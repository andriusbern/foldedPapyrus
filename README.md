



# <img src="https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/320/softbank/145/scroll_1f4dc.png" alt="drawing" width="25"/> **Folded Papyrus** 


This dataset will contain AlphaFold's predictions and internal embeddings for most* of the proteins that are referenced in Papyrus.

**Note: At the current date **(08/06/2022) - 3423/6000+** proteins contained in Papyrus dataset are fully processed. Some of the longer sequences (5000+ amino acids) might remain off-limits with the available hardware).*

Why is this dataset useful?
- Even though an extensive database of AlphaFold’s predictions of protein 3D structures is already publicly available, the features that the model uses internally have not yet been published. Since the final output of AlphaFold is the set of 3D coordinates of the protein backbone, one could argue that this format is highly condensed (given the huge scale-down in dimensionality). It is likely that there is a lot more valuable information that can be captured within this large transformer model.
- Additionally, since processing multiple proteins through this model takes a considerable amount of time and compute, doing it multiple times is somewhat wasteful. After processing a protein sequence the generated embeddings and predictions can be saved and reused for other projects.


<br /> 

---
## **1. Quick description of AlphaFold**



Definitions for each internal representation of proteins used within AlphaFold can be found in the [supplementary material](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM1_ESM.pdf) of the AlphaFold publication. Below, each representation will be referred to using the same notation as in the paper.

![https://elearning.bits.vib.be/wp-content/uploads/2021/12/architecture.png](https://elearning.bits.vib.be/wp-content/uploads/2021/12/architecture.png)

### **Three main components:**

   <br /> 

### **B.** Database search & preprocessing (CPU and hard disk I/O intensive):
   1. MSA module queries protein databases to find similar amino acid sequences, performs multiple sequence alignment and creates the initial ***MSA representation*  $(m_{si})$.** This contains all the relevant information about evolutionarily related protein sequences.
   2. ***Pair representation*** $(z_{ij})$ is created by querying protein *structure* databases, trying to find similar proteins with known structure and then building feature vectors for each residue pair based on this information.
   <br /> 

### **C.** Prediction model (GPU):
   1. Evoformer - a very large transformer that iteratively cross-integrates information from the ***MSA representation $(m_{si})$*** and ***pair representation $(z_{ij})$*** using a wide variety of attention mechanisms. One of its outputs is the **single representation** $(s_i)$ which condenses all the MSA information into a single vector (per residue).
   2. Structure module - a point invariant transformer that uses features generated by the evoformer to iteratively modify a 3D graph of the protein backbone and produce the final prediction. After the last layer of this transformer another **structure representation  $(a_i)$** can be exctracted. 
   

   
### **D.** Amber relaxation (CPU/GPU):
1.  Makes sure that there are no violations and clashes in the generated 3D structure and attempts to fix it if needed.


<br /> 

---
## **2. Dataset contents**

Folder contents for each processed protein are listed below:


&#8595; **data/*** -> main data directory

> &#8595; **data/PID/***         -> folder of a single protein
>
> > 
> > | Filename | Description | Tensor shape |
> > | --- | --- | --- |
> > | **msa.npy***         | $(m_{si})$ processed MSA representation            |  *[num_aligned_seqs x **L** x 256]* 
> > | **single.npy***      | ($s_i$)  evoformer single representation   | *[**L** x 384]* |   
> > | **pair.npy***        |  $(z_{ij}$) evoformer pair representation  |  *[**L** x **L** x 128]* | 
> > | **structure.npy***   | $(a_i)$  output of the last layer of structure module  | *[**L** x 384]* | 
> > | **lldt.npy**        | ($s_i$)  evoformer single representation   | *[**L** x 384]* |   
> > | **conf.npy**        |  $(z_{ij}$) evoformer pair representation  |  *[**L** x **L** x 128]* | 
> > | **sac.npy**   | $(a_i)$  output of the last layer of structure module  | *[**L** x 384]* | 
> > | **PID.pdb**     | 3D protein structure prediction | | |
> > |**PID.fasta**    | fasta file containing the amino acid sequence of that protein | | |
> > | **features.pkl**    | Various additional metrics that AlphaFold saves automatically   
> > | **timings.json**    | Processing log 
> &#8595; **data/PID2/***  -> folder of a single protein #2
> > 
> > **...**
---
*Note: **PID** refers to the protein accesion number, **L** - number of residues in the protein sequence.*

<br /> 

---
## **3. How to download**

First clone the repository:

```bash
git clone URL && cd foldedPapyrus
```

1. **Single proteins**

     Data of individual proteins can be downloaded using their accession numbers.

    ```bash
    python download.py --pid P14324
    ```

2. **Partial dataset**
    
    You can use a file containing a list of protein accession numbers separated by a newline **“\n”** and pass it via command line. You can find examples of these protein subset files in the ***subsets/*** directory.
    
    ```bash
    python download.py --pid_file subsets/kinases.txt
    ```
    
3. **Full dataset**

     Alternatively if you want to download all the data that is currently available:

    ```bash
    python download.py --all
    ```

<br /> 

<!-- ---
## **4. Sample code**

This repository also contains some sample code for using this dataset:

1. **utils/transformer.py**
    - Implementation of a basic transformer encoder that could be used directly with $(s_i)$ and $(a_i)$ representations. Note that this is only a template as there is no real training objective defined in this model.
2. **utils/dataset.py**
    - Contains a subclassed **PyTorch Dataset** class for loading this data.
3. **utils/train.py**
    - Contains a backbone for training the transformer model. 
4. **utils/explore.ipynb**
    - This notebook contains:
        - Functions for displaying proteins (using PyMOL)
        - Generating some statistics for your data subset
        - Training a base model
-->

<br /> 

---
## Notes

1. AlphaFold actually consists of 5 separate models with slightly different architectures and hyperparameters. The normal pipeline of processing a single protein involves all 5 of these models whose predictions are then ranked based on the confidence metrics. Since this requires 5x the time/compute, this dataset was created by only running the [Model 0]. In the future this dataset could be expanded to include predictions from all models (if there is a need for it).
2. This dataset was produced by modifying AlphaFold code, parallelising it and optimising the throughput of CPU and GPU intensive parts of the code. By now the pipeline is very automated so if you need embeddings for specific proteins, please contact me at a.bernatavicius@liacs.leidenuniv.nl, I will be glad to help.
3. At the current date (08/06/2022) - **3423/6231** proteins contained in Papyrus dataset are fully processed.
 Some of the longer sequences (5000+ amino acids) might remain off-limits with the available hardware).

---